{
  "name": "Speedo",
  "tagline": "Parallelizing Stochastic Gradient Descent for Deep Convolutional Neural Network",
  "body": "# SpeeDo - Parallelizing Stochastic Gradient Descent for Deep Convolutional Neural Network\r\n\r\n## Introduction\r\n\r\nConvolutional Neural Networks (CNNs) have achieved breakthrough results on many machine learning tasks. However, training CNNs is computationally intensive. When the size of training data is large and the depth of CNNs is high, as typically required for attaining high classification accuracy, training a model can take days and even weeks. So we propose [SpeeDO](http://learningsys.org/papers/LearningSys_2015_paper_13.pdf) (for Open DEEP learning System in backward order), a deep learning system designed for off-the-shelf hardwares. SpeeDO can be easily deployed, scaled and maintained in a cloud environment, such as AWS EC2 cloud, Google GCE, and Microsoft Azure.\r\n\r\nIn our implement, we support 5 distributed SGD models to speed up the training:\r\n\r\n* Synchronous SGD\r\n* Asynchronous SGD\r\n* Partially Synchronous SGD\r\n* Weed-Out SGD\r\n* Elastic Averaging SGD\r\n\r\nPlease cite [SpeeDO](http://learningsys.org/papers/LearningSys_2015_paper_13.pdf) in your publications if it helps your research:\r\n\r\n    @article{zhengspeedo,\r\n      title={SpeeDO: Parallelizing Stochastic Gradient Descent for Deep Convolutional Neural Network},\r\n      author={Zheng, Zhongyang and Jiang, Wenrui and Wu, Gang and Chang, Edward Y}\r\n    }\r\n\r\n## Architecture\r\n\r\nSpeeDO takes advantage of many existing solutions in the open-source community, data flow of SpeeDO:\r\n\r\n![Architecture and data flow of SpeeDO](https://raw.githubusercontent.com/openbigdatagroup/speedo/master/data/figures/speedo_architecture.png \"Architecture and data flow of SpeeDO\")\r\n\r\nSpeeDO mainly contains these components:\r\n\r\n* [Caffe](http://caffe.berkeleyvision.org/) (required)\r\n* [Redis](http://redis.io) (required)\r\n* [Akka](http://akka.io)  (required)\r\n* [Yarn](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)  [optional]\r\n* [HDFS](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)  [optional]\r\n\r\nThese components denote what we need to deploy before the distributed training.\r\n\r\n# Deploy and Run\r\n\r\n## Pre-requisite\r\n\r\n* JDK 1.7+\r\n* [Redis Server](http://redis.io/download)\r\n* Ubuntu 12.04+ (not test on other linux os family)\r\n* Network connection: You need to connect to maven and ivy repositories when compiling the demo\r\n\r\nSpeeDO is running in **Master-Slaves(Worker)** archiecture. To avoid manual process in running, and distribute input data in Master and worker nodes, we can use YARN and HDFS. In below, we provides the instruction to run SpeeDO for both scenarios.\r\n\r\n| Configuration | YARN present | HDFS present |\r\n| ------------- | ------------- | ------------- |\r\n| A | N | N |\r\n| B | Y | Y |\r\n\r\n**NOTE**\r\n\r\ni. YARN is used for nodes resource scheduling. If YARN is not present, we can run our Master , and Worker process manually.\r\n\r\nii. HDFS is used for storing training data and network definition of caffe. If HDFS is not present, we can use shared-files system (like NFS ) or manually copying these files to each nodes.\r\n\r\nWe provide the steps to run configuration **A** and **B** here:\r\n* For configuration **A**, we manually deploy and run on all nodes.\r\n* For configuration **B**, we use [cloudera](http://www.cloudera.com/documentation/manager/5-1-x/Cloudera-Manager-Installation-Guide/Cloudera-Manager-Installation-Guide.html) (offering us both YARN and HDFS) to deploy and run SpeeDO.\r\n\r\n## A. Deploy and run SpeeDO without YARN and HDFS\r\n\r\nWe provides TWO methods here: 1) Docker , 2) Manual ( step by step)\r\n\r\n##1. Quick Start ( via Docker )\r\n\r\n### Step.0 Pull image\r\nPull the speedo image ( bundled with caffe and all its dependencies libraries):\r\n```bash\r\ndocker pull openbigdatagroup/speedo:latest\r\n```\r\n\r\n### Step.1 Run containers on cluster\r\nThe following example will run 1000 iterations asynchronously using 1 Master with 3 workers ( 4 cluster nodes )\r\n\r\n#### Master\r\nLaunch master container on your master node (in default Async model with 3 workers):\r\n```bash\r\ndocker run -d --name=speedo-master --net=host openbigdatagroup/speedo\r\n```\r\n\r\n**Or** run master actor in Easgd model with 3 workers\r\n```bash\r\ndocker run -d --name=speedo-master --net=host openbigdatagroup/speedo master <master-address> 3 --test 0 --maxIter 1000 --movingRate 0.5\r\n```\r\n\r\nPlease replaces `master-address` with master node's ip\r\n\r\n**NOTE**\r\nRedis service will be started automatically when launching master container\r\n\r\n#### Worker\r\nLaunch 3 worker containers on different worker nodes:\r\n```bash\r\ndocker run -d --name=speedo-worker --net=host openbigdatagroup/speedo worker <master-address> <worker-address>\r\n```\r\n\r\nPlease replaces `master-address` with master node's ip, and `worker-address` with the current worker node's ip\r\n\r\n##2. Manually ( Step by Step )\r\n\r\n### Step.0 Pre-requistie\r\nInstall at each nodes ( Master and Worker)\r\n1. JDK 1.7+\r\n2. Redis Server\r\n3. Clone SpeeDO and Caffe source from our github repo\r\n\r\nPlease use\r\n```\r\ngit clone --recursive git@github.com/openbigdatagroup/speedo.git # SpeeDO and caffe\r\n```\r\n\r\n### Step.1 Install caffe and its dependencies\r\nInstall [speedo/caffe](https://github.com/openbigdatagroup/caffe) and all its dependencies on each nodes , please refer to section **A. Manually install on all cluster nodes** from [speedo/caffe install guide](https://github.com/openbigdatagroup/caffe).\r\n\r\n### Step.2 Prepare Input Data to run under Caffe\r\n\r\n**NOTE**: We prefer to use **datumfile** format for SpeeDO ( see [caffe-pullrequest-2193](https://github.com/BVLC/caffe/pull/2193) ) instead of the default leveldb/lmdb format during training in Caffe to solve the memory usage problem ( refer to [caffe-issues-1377](https://github.com/BVLC/caffe/issues/1377)).\r\n\r\nThe input data required by Caffe, including:\r\n* solver definition\r\n* network definition\r\n* training datasets\r\n* testing datasets\r\n* mean values\r\n\r\nIn this example, let's train cifar10 dataset and generate `training datasets` and `testing datasets` in dataumfile format:\r\n```bash\r\ncd caffe\r\n./data/cifar10/get_cifar10.sh # download cifar dataset\r\n./examples/speedo/create_cifar10.sh  # create protobuf file - in datumfile instead of leveldb/lmdb format\r\n```\r\n\r\n`Solver definition`, `network definition` and `means values` written in datumfile format for cifar10 is provided at examples/speedo.\r\n\r\n>  If you want to manually produce these files, please follow the steps below. (Modify all paths in network definitions if needed ) :\r\n```bash\r\nsed -i \"s/examples\\/cifar10\\/mean.binaryproto/mean.binaryproto/g\" cifar10_full_train_test.prototxt\r\nsed -i \"s/examples\\/cifar10\\/cifar10_train_lmdb/cifar10_train_datumfile/g\" cifar10_full_train_test.prototxt\r\nsed -i \"s/examples\\/cifar10\\/cifar10_test_lmdb/cifar10_test_datumfile/g\" cifar10_full_train_test.prototxt\r\nsed -i \"s/backend: LMDB/backend: DATUMFILE/g\" cifar10_full_train_test.prototxt\r\nsed -i \"17i\\    rand_skip: 50000\" cifar10_full_train_test.prototxt\r\nsed -i \"s/examples\\/cifar10\\/cifar10_full_train_test.prototxt/cifar10_full_train_test.prototxt/g\" cifar10_full_solver.prototxt\r\n```\r\n\r\nAt last, put the data in the same location(like /tmp/caffe/cifar10) on **all Master and Workers node**. You can do that by [Ansible](https://www.ansible.com/) or just scp to the right location.\r\n\r\n### Step.3  Training under SpeeDO\r\n\r\nSpeeDO use Master + Worker archiecture for the distributed training (Please refer to our paper for the detail information). We need to start Master node and Worker node as below.\r\n\r\n#### Compile bundle jar\r\nOn each master and worker nodes, run\r\n```bash\r\ngit clone git@github.com/openbigdatagroup/speedo.git # if not done yet\r\ncd speedo\r\n./sbt akka:assembly\r\n```\r\n\r\n#### Run Master and Worker process\r\n\r\nThe following example will run 1000 iterations asynchronously using 1 Master with 3 workers ( 4 cluster nodes ).\r\n\r\n##### Master\r\nLaunch master process on your master node:\r\n```bash\r\nJAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:/usr/lib java -cp target/scala-2.11/SpeeDO-akka-1.0.jar -Xmx2G com.htc.speedo.akka.AkkaUtil --solver /absolute_path/to/cifar10_full_solver.prototxt --worker 3 --redis <redis-address> --test 500 --maxIter 1000 --host <master-address> 2> /dev/null\r\n```\r\n\r\nPlease replaces `redis-address` with the redis server location, and `master-address` with master node's ip/hostname.\r\n\r\nThis should output some thing like:\r\n\r\n    [INFO] [03/03/2016 15:07:41.626] [main] [Remoting] Starting remoting\r\n    [INFO] [03/03/2016 15:07:41.761] [main] [Remoting] Remoting started; listening on addresses :[akka.tcp://SpeeDO@cloud-master:56126]\r\n    [INFO] [03/03/2016 15:07:41.763] [main] [Remoting] Remoting now listens on addresses: [akka.tcp://SpeeDO@cloud-master:56126]\r\n    [INFO] [03/03/2016 15:07:41.777] [SpeeDO-akka.actor.default-dispatcher-3] [akka.tcp://SpeeDO@cloud-master:56126/user/host] Waiting for 3 workers to join.\r\n\r\n##### Worker\r\nLaunch 3 workers process on worker nodes:\r\n```bash\r\nJAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:/usr/lib java -cp target/scala-2.11/SpeeDO-akka-1.0.jar -Xmx2G com.htc.speedo.akka.AkkaUtil --host <worker-address> --master <masteractor-addr> 2> /dev/null\r\n```\r\n\r\nPlease replaces `worker-address` with worker's ip/hostname,  and `masteractor-addr` with master actor address.\r\n\r\nThe format of master actor address is **`akka.tcp://SpeeDO@cloud-master:56126/user/host`**, where cloud-master is the hostname of master node, and 56126 is the TCP port listen by akka's actor. Since the port is random by default, the address can vary in different runs. You can also use fixed port by passing a `--port <port>` command line argument when start Master.\r\n\r\n\r\n\r\n## B. Deploy and run SpeeDO by cloudera\r\nTo try a cloudera solution for SpeeDO. Please refer [Run SpeeDO on Yarn & HDFS Cluster](https://github.com/openbigdatagroup/speedo/blob/master/README_YARN.md)\r\n\r\n## Experiments on AWS\r\n\r\nThe Cifar10 dataset is used to validate all parallel implementations on a CPU cluster with four 8-core instances\r\n\r\n![SGD parallel schemes on CPU cluster](https://raw.githubusercontent.com/openbigdatagroup/speedo/master/data/figures/speedo_psgd_cpu.png \"SGD parallel schemes on CPU cluster\")\r\n\r\nTraining [GoogleNet](http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf) on a GPU cluster for different parallel implementations\r\n\r\n![SGD parallel schemes on GPU cluster](https://raw.githubusercontent.com/openbigdatagroup/speedo/master/data/figures/speedo_psgd_gpu.png \"SGD parallel schemes on GPU cluster\")\r\n\r\nEASGD achieves the best speedup in our parallel implementations. And parameters of it have great impact for the speedup.\r\n\r\n![Parameter Analysis of EASGD on GPU Cluster](https://raw.githubusercontent.com/openbigdatagroup/speedo/master/data/figures/speedo_easgd_gpu.png \"Parameter Analysis of EASGD on GPU Cluster\")\r\n\r\n## Authors\r\n\r\n* [Zhongyang Zheng](https://github.com/zyzheng)\r\n* [Wenrui Jiang](https://github.com/wenruij)\r\n* [Gang Wu](https://github.com/simonandluna)\r\n\r\n## Supervisor\r\n* [Edward Y. Chang](http://infolab.stanford.edu/~echang/)\r\n\r\n## License\r\n\r\nCopyright 2016 HTC Corporation\r\n\r\nLicensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}
